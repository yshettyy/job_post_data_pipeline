# Project Name

The project is about utilizing Big Data technologies so to develop a pipeline for extracting job posting in linkedin. 

## Table of Contents

- [About](#about)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- 

## About

Project demonstrates the etl process used on Big data technologies like HDFS, Hue, Hive and Spark to extract linkedin job posting via an API with the help of a workflow orchestration tool Airflow.

## Getting Started

Follow the given step to run the technologies in your local system.

### Prerequisites

Before you begin make sure, you have docker installed with 8 gb of ram provided.

### Installation

Thanks to Marc Lamberti 
The whole docker setup is provided by him like the mapping with the local system, internal connection between the different technologies. 

./start.sh
To install all the technologies inside your docker environment.
